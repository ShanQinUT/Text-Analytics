{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin=pd.read_csv('Austin_Giorgio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      sound exactly like original song wanna hear th...\n",
       "1                               cute nothing spectacular\n",
       "2      honestly song needed wasnt going win crooning ...\n",
       "3                     love voice austin giorgiodont give\n",
       "4                                           ugh suit üòçüòçüòç\n",
       "5                               sound like michael buble\n",
       "6                                            damn boring\n",
       "7      really think awful song choice style voice uni...\n",
       "8      think song choice wise one showcase voice way ...\n",
       "9           man decepcion playoff think would pas top 12\n",
       "10                                                hot af\n",
       "11                                            big nooooo\n",
       "12                  michael buble justin bieber canadian\n",
       "13                                         ok wrong song\n",
       "14               sweet tasty guitar part need learn that\n",
       "15                                 ruined beautiful song\n",
       "16                                      he handsome cant\n",
       "17            literally sound exactly like michael buble\n",
       "18                                       shouldve stayed\n",
       "19     like blake now pick austinüò≠üò≠üò≠üò≠ extremely wante...\n",
       "20                                     didnt go throughüò≠\n",
       "21     he got wonderful voice prefer old timey song c...\n",
       "22     think guy talented however buble seems teeteri...\n",
       "23     sorry think blake made big mistake keeping you...\n",
       "24     okhe much better vocal thisto bad song choiceh...\n",
       "25                 wow man singing man song love concept\n",
       "26                                          ok glad gone\n",
       "28                                   itunes release song\n",
       "29                                 bad choice him jeez üò±\n",
       "30     needed see range genre could do dont like jb g...\n",
       "                             ...                        \n",
       "211    totally preferred jazz side good he showing pi...\n",
       "212    confusing many levelsthe song choice turtlenec...\n",
       "218    literally feel like im watching entirely diffe...\n",
       "220    naturally beautiful tone lotta style looking g...\n",
       "223                                    want itunes badly\n",
       "224    covered song youtube channel httpsyoutubepdh1m...\n",
       "225    pick anything thats become gold teamaustingiorgio\n",
       "226                       nicely tailored suit thats all\n",
       "227                             blake trying get rid him\n",
       "232                     always wanted spin pop song cool\n",
       "233    song make sound like mark isaiah dont go mark ...\n",
       "234                              see eye didnt want song\n",
       "236                                             boring üíØ\n",
       "237                                                  meh\n",
       "238                   seems like bussing already started\n",
       "240                       kinda sound like michael bubl√©\n",
       "250                                      bad song choice\n",
       "252    woah surprising song choice still soooo good n...\n",
       "257    ive waiting forthat little twistonce loosen li...\n",
       "258                expected sinatra instead got bieber üò™\n",
       "264      austin sound like dez duron sings justin bieber\n",
       "265                                            that good\n",
       "267                                  great job austin ‚ù§Ô∏è\n",
       "268                                  lmfao boy pick song\n",
       "274                              idk sound kinda awkward\n",
       "275    bro wanted hear dude sing frank sinatra song g...\n",
       "285                      early people view comment early\n",
       "286                                            good song\n",
       "287                   think best preformance still good‚ù§\n",
       "289                                                first\n",
       "Name: commentText, Length: 214, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "austinnona=austin['commentText'][pd.notna(austin['commentText'])]\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in str(doc).lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    other_punc_free=''.join(ch for ch in punc_free if ch!='‚Äô')\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in other_punc_free.split())\n",
    "    return normalized\n",
    "austincom=austinnona.map(clean)\n",
    "austincom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "austincomtoken=austincom.map(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'austin.comwords=[]\\ndef makelist(s):\\n    for i in range(len(s)):\\n        return austin.comwords.append(s[i])\\n\\naustincomtoken.map(makelist)\\naustin.comwords'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''austin.comwords=[]\n",
    "def makelist(s):\n",
    "    for i in range(len(s)):\n",
    "        return austin.comwords.append(s[i])\n",
    "\n",
    "austincomtoken.map(makelist)\n",
    "austin.comwords'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(austincomtoken)\n",
    "corpus=[id2word.doc2bow(text) for text in austincomtoken]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.019*\"song\" + 0.018*\"performance\" + 0.013*\"austin\" + 0.011*\"like\" + 0.009*\"far\" + 0.009*\"why\" + 0.009*\"amazing\" + 0.009*\"love\" + 0.007*\"cute\" + 0.007*\"voice\" + 0.007*\"go\" + 0.007*\"omg\"')\n",
      "(1, '0.055*\"song\" + 0.020*\"good\" + 0.018*\"like\" + 0.017*\"choice\" + 0.014*\"him\" + 0.011*\"voice\" + 0.010*\"love\" + 0.009*\"it\" + 0.009*\"show\" + 0.008*\"really\" + 0.008*\"bad\" + 0.007*\"turtleneck\"')\n",
      "(2, '0.030*\"song\" + 0.021*\"austin\" + 0.014*\"think\" + 0.013*\"bieber\" + 0.013*\"choice\" + 0.012*\"buble\" + 0.011*\"best\" + 0.010*\"good\" + 0.010*\"sound\" + 0.010*\"blake\" + 0.009*\"love\" + 0.009*\"im\"')\n",
      "(3, '0.025*\"like\" + 0.017*\"michael\" + 0.015*\"song\" + 0.014*\"love\" + 0.013*\"buble\" + 0.011*\"sing\" + 0.009*\"austin\" + 0.008*\"sound\" + 0.007*\"way\" + 0.007*\"get\" + 0.007*\"blake\" + 0.007*\"make\"')\n"
     ]
    }
   ],
   "source": [
    "NUM_TOPICS = 4\n",
    "\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=id2word, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=12)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.019*\"artist\" + 0.016*\"like\" + 0.013*\"voice\" + 0.010*\"it\" + 0.010*\"want\" + 0.010*\"sung\" + 0.010*\"performed\" + 0.010*\"everyone\" + 0.007*\"show\" + 0.007*\"little\" + 0.007*\"tonight\" + 0.007*\"loved\"')\n",
      "(1, '0.031*\"song\" + 0.025*\"country\" + 0.024*\"love\" + 0.019*\"voice\" + 0.016*\"spensha\" + 0.012*\"girl\" + 0.012*\"need\" + 0.012*\"music\" + 0.012*\"like\" + 0.010*\"make\" + 0.010*\"right\" + 0.010*\"choice\"')\n",
      "(2, '0.019*\"voice\" + 0.015*\"blake\" + 0.014*\"good\" + 0.012*\"country\" + 0.012*\"feel\" + 0.011*\"amazing\" + 0.011*\"spensha\" + 0.010*\"really\" + 0.008*\"thought\" + 0.008*\"genre\" + 0.008*\"fact\" + 0.008*\"team\"')\n",
      "(3, '0.031*\"song\" + 0.024*\"blake\" + 0.020*\"better\" + 0.016*\"voice\" + 0.015*\"think\" + 0.015*\"bad\" + 0.015*\"like\" + 0.013*\"singer\" + 0.011*\"choice\" + 0.011*\"it\" + 0.009*\"good\" + 0.009*\"one\"')\n"
     ]
    }
   ],
   "source": [
    "spensha=pd.read_csv('Spensha_Baker.csv')\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "spenshanona=spensha['commentText'][pd.notna(spensha['commentText'])]\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in str(doc).lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    other_punc_free=''.join(ch for ch in punc_free if ch!='‚Äô')\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in other_punc_free.split())\n",
    "    return normalized\n",
    "spenshacom=spenshanona.map(clean)\n",
    "spenshacomtoken=spenshacom.map(word_tokenize)\n",
    "id2word = corpora.Dictionary(spenshacomtoken)\n",
    "corpus=[id2word.doc2bow(text) for text in spenshacomtoken]\n",
    "\n",
    "NUM_TOPICS = 4\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=id2word, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=12)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.028*\"wilkes\" + 0.019*\"song\" + 0.017*\"performance\" + 0.016*\"great\" + 0.013*\"choice\" + 0.013*\"he\" + 0.013*\"amazing\" + 0.013*\"save\" + 0.011*\"im\" + 0.011*\"really\" + 0.010*\"blake\" + 0.010*\"top\"')\n",
      "(1, '0.048*\"mic\" + 0.024*\"drop\" + 0.021*\"love\" + 0.013*\"song\" + 0.013*\"performance\" + 0.013*\"wilkes\" + 0.011*\"like\" + 0.010*\"get\" + 0.010*\"voice\" + 0.009*\"hope\" + 0.009*\"singer\" + 0.008*\"make\"')\n",
      "(2, '0.032*\"wilkes\" + 0.014*\"voice\" + 0.014*\"like\" + 0.012*\"blake\" + 0.010*\"he\" + 0.010*\"win\" + 0.009*\"song\" + 0.008*\"save\" + 0.008*\"favorite\" + 0.008*\"cant\" + 0.008*\"love\" + 0.007*\"good\"')\n",
      "(3, '0.024*\"wilkes\" + 0.013*\"go\" + 0.012*\"make\" + 0.011*\"performance\" + 0.010*\"he\" + 0.010*\"really\" + 0.010*\"vote\" + 0.010*\"song\" + 0.010*\"good\" + 0.010*\"mic\" + 0.009*\"12\" + 0.009*\"top\"')\n"
     ]
    }
   ],
   "source": [
    "wilkes=pd.read_csv('Wilkes.csv')\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "wilkesnona=wilkes['commentText'][pd.notna(wilkes['commentText'])]\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in str(doc).lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    other_punc_free=''.join(ch for ch in punc_free if ch!='‚Äô')\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in other_punc_free.split())\n",
    "    return normalized\n",
    "wilkescom=wilkesnona.map(clean)\n",
    "wilkescomtoken=wilkescom.map(word_tokenize)\n",
    "id2word = corpora.Dictionary(wilkescomtoken)\n",
    "corpus=[id2word.doc2bow(text) for text in wilkescomtoken]\n",
    "\n",
    "NUM_TOPICS = 4\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=id2word, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=12)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
